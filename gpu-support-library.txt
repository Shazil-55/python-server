CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir

// instead of using cpp-python write this  command for Cuda GPU support 